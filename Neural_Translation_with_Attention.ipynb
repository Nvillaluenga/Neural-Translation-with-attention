{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural-Translation-with-Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNedCDhpwe+C6GNhq6mc1NO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nvillaluenga/Neural-Translation-with-attention/blob/NachDev/Neural_Translation_with_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00428CVZZAX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59fb3ab3-c35d-4bae-d989-ef8af04c4ce8"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__) # as TF is v=2.3.0 eager execution is enabled by default"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfdhS_o1blXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "129b993c-ef15-4076-e0db-363c4fedb245"
      },
      "source": [
        "# Get our data\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    fname='spa-eng.zip',\n",
        "    origin='http://download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "print(path_to_zip)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+'/spa-eng/spa.txt'\n",
        "print(path_to_file)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/spa-eng.zip\n",
            "/root/.keras/datasets/spa-eng/spa.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epzo0dTcg5UH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing functions\n",
        "\n",
        "def unicode_to_ascii(string):\n",
        "  return ''.join( c for c in unicodedata.normalize('NFD', string)\n",
        "    if unicodedata.category(c) != 'Mn' )\n",
        "  \n",
        "def preprocess_sentence(string):\n",
        "  string = unicode_to_ascii(string.lower().strip())\n",
        "  # separate words from simbols (?.!,¿), eliminate double (or more) spaces\n",
        "  # and replace every weird stuff non english character with space\n",
        "  string = re.sub(r'([?.!,¿])', r' \\1', string)\n",
        "  string = re.sub(r'[\" \"]+', ' ', string)\n",
        "  string = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', string)\n",
        "  string = string.rstrip().strip()\n",
        "  # adding a start and an end token to the sentence\n",
        "  string = '<start> ' + string + ' <end>'\n",
        "  return string\n",
        "\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [ [ preprocess_sentence(word) for word in line.split('\\t') ]\n",
        "                for line in lines[:num_examples] ]\n",
        "  return word_pairs\n",
        "\n",
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa\n",
        "class LanguageIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "\n",
        "    self.create_index()\n",
        "  \n",
        "  def create_indec():\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase.split(' '))\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "\n",
        "    self.word2idx['<pad>'] = 0\n",
        "    self.idx2word[0] = '<pad>'\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index+1\n",
        "      self.idx2word[index+1] = word\n",
        "\n",
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "def load_dataset(path, num_examples):\n",
        "  # Create input output pairs\n",
        "  pairs = create_dataset(path, num_examples)\n",
        "\n",
        "  inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
        "  target_lang = LanguageIndex(en for en, sp in pairs)\n",
        "\n",
        "  # Vectorize all this\n",
        "  input_tensor = [ [ inp_lang.word2idx[word] for word in sp.split(' ') ]\n",
        "                  for en, sp in pairs ]\n",
        "  target_tensor = [ [ target_lang.word2idx[word] for word in en.split(' ') ]\n",
        "                  for en, sp in pairs ]\n",
        "\n",
        "  # Padding input and output\n",
        "  max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "  input_tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      sequences = input_tensor,\n",
        "      maxlen = max_length_inp,\n",
        "      padding = 'post'\n",
        "  )\n",
        "  target_tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      sequences = target_tensor,\n",
        "      maxlen = max_length_tar,\n",
        "      padding = 'post'\n",
        "  )\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang, tar_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W9hMpysj4a6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MwFmLwLgqPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Limit the size of the dataset to experiment faster (optional)\n",
        "\n",
        "..."
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}